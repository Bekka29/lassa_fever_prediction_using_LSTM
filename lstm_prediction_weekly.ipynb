{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import pandas as pd\n",
    "from pandas import read_csv, concat, DataFrame\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the Seaborn context to 'talk' and style to 'whitegrid'\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel file\n",
    "excel_file = 'data/wlfdata.xlsx'\n",
    "df = pd.read_excel(excel_file, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file\n",
    "csv_file = 'data/wlfdata.csv'\n",
    "df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_csv('data/wlfdata.csv')\n",
    "print(dataset.head())\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cases in each year\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "years = np.arange(2018, 2023, 1).astype(int)\n",
    "for year in years:\n",
    "    sns.lineplot(data=dataset[dataset.year == year],\n",
    "                    x=\"epiweek\", y='cases', ax=ax, label=year)\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Cases')\n",
    "    ax.set_title('Total Lassa Cases on a Yearly Basis')\n",
    "fig.patch.set_alpha(0)\n",
    "plt.legend(bbox_to_anchor=(1.15, 1), loc=\"upper right\")\n",
    "fig.savefig('cases_by_years.png', dpi=300)\n",
    "print(\"File saved successfully as 'cases_by_years.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df contains the data with columns: 'State', 'Year', 'Month', and numeric features\n",
    "\n",
    "# Define the figure and axes for plotting\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Plot the data for each state\n",
    "for i, state in enumerate(dataset['state'].unique()):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax = axes[row, col]\n",
    "    state_data = dataset[dataset['state'] == state]\n",
    "    sns.lineplot(data=state_data, x='epiweek', y='cases', hue='year', ax=ax)\n",
    "    ax.set_title(f'State: {state}')\n",
    "    ax.set_xlabel('Epiweek')\n",
    "    ax.set_ylabel('Cases')\n",
    "    ax.legend(title='Year')\n",
    "    ax.tick_params(axis='x', labelrotation=90)  # Rotate x-axis labels vertically\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set this 'date' column as the index if needed\n",
    "dataset.set_index('datetime', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['year', 'epiweek','state'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat Map showing the correlation between all variables including the target\n",
    "corr=dataset.corr(method='spearman').abs()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "matrix = np.triu(corr) # Getting the lower traingle of the correlation matrix\n",
    "cbar_kws={\"label\": \"Correlation\", \"shrink\":1}\n",
    "heatmap=sns.heatmap(data=corr, linewidths=1, square=False, cmap='Reds', ax=ax, annot=True,annot_kws={\"size\": 10}, mask=matrix, fmt= \".2f\",cbar_kws=cbar_kws)\n",
    "fig.suptitle('Heatmap of Correlation Between Data Features', fontsize=18, y=.94, x=.43);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "# specify columns to plot\n",
    "groups = [0, 1, 2, 3, 4, 5, 6]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize=(10,15))\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or Pandas DataFrame.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = [], []\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(str(df.columns[j]) + '(t-%d)' % i) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n_out-1)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(str(df.columns[j]) + '(t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(str(df.columns[j]) + '(t+%d)' % i) for j in range(n_vars)]\n",
    "\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "values = dataset.values\n",
    "encoder = LabelEncoder()\n",
    "values[:, 6] = encoder.fit_transform(values[:, 6])\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_weeks = int(len(values) * 0.8)\n",
    "train = values[:n_train_weeks, :]\n",
    "test = values[n_train_weeks:, :]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "print(\"Shape of train_X:\", train_X.shape)\n",
    "print(\"Shape of train_y:\", train_y.shape)\n",
    "print(\"Shape of test_X:\", test_X.shape)\n",
    "print(\"Shape of test_y:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# Fit network\n",
    "history = model.fit(train_X, train_y, epochs=100, batch_size=52, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "# Plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging shapes before prediction\n",
    "print(\"Shape of test_X before prediction:\", test_X.shape)\n",
    "\n",
    "# Make a prediction\n",
    "try:\n",
    "    yhat = model.predict(test_X)\n",
    "    print(\"Shape of yhat:\", yhat.shape)\n",
    "except Exception as e:\n",
    "    print(\"Error during prediction:\", e)\n",
    "    print(\"Input shape to model:\", test_X.shape)\n",
    "\n",
    "# Reshape test_X back to its original shape for inverse scaling\n",
    "test_X_flat = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "# Debugging shapes after prediction\n",
    "print(\"Shape of yhat:\", yhat.shape)\n",
    "print(\"Shape of test_X after reshape:\", test_X_flat.shape)\n",
    "\n",
    "# Create an array of zeros to concatenate with yhat for inverse scaling\n",
    "# Adjusting the shapes to match the scaled data\n",
    "inv_yhat_full = np.zeros((len(yhat), scaled.shape[1]))\n",
    "inv_yhat_full[:, 0] = yhat[:, 0]  # Put the predictions in the first column\n",
    "# Ensure the remaining part of inv_yhat_full matches test_X_flat\n",
    "inv_yhat_full[:, 1:] = test_X_flat[:, :scaled.shape[1] - 1]  # Adjusting to the correct number of columns\n",
    "\n",
    "# Invert scaling for forecast\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat_full)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "\n",
    "# Invert scaling for actual\n",
    "test_y_full = np.zeros((len(test_y), scaled.shape[1]))\n",
    "test_y_full[:, 0] = test_y\n",
    "# Ensure the remaining part of test_y_full matches test_X_flat\n",
    "test_y_full[:, 1:] = test_X_flat[:, :scaled.shape[1] - 1]  # Adjusting to the correct number of columns\n",
    "\n",
    "inv_y = scaler.inverse_transform(test_y_full)\n",
    "inv_y = inv_y[:, 0]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
